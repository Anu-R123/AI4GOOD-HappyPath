{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet50v2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pS-ifLRnMYM3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Flatten, Dense, Input, Lambda, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, preprocess_input\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import load_img,ImageDataGenerator, img_to_array\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import os\n",
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = [150,150]\n",
        "train_path = ''\n",
        "valid_path = ''"
      ],
      "metadata": {
        "id": "VXUzGZXhMvNF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set your seeds to match each other, so your training and validation sets don't overlap\n",
        "# BATCH_SIZE = 32\n",
        "# IMG_SIZE = [150,150]\n",
        "# directory = \"dataset/\"\n",
        "# train_dataset = image_dataset_from_directory(directory,\n",
        "#                                              shuffle=True,\n",
        "#                                              batch_size=BATCH_SIZE,\n",
        "#                                              image_size=IMG_SIZE,\n",
        "#                                              validation_split=0.2,\n",
        "#                                              subset='training',\n",
        "#                                              seed=42)\n",
        "# validation_dataset = image_dataset_from_directory(directory,\n",
        "#                                              shuffle=True,\n",
        "#                                              batch_size=BATCH_SIZE,\n",
        "#                                              image_size=IMG_SIZE,\n",
        "#                                              validation_split=0.2,\n",
        "#                                              subset='validation',\n",
        "#                                              seed=42)"
      ],
      "metadata": {
        "id": "2bZDdMRsNFST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class_names = train_dataset.class_names\n",
        "\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# for images, labels in train_dataset.take(1):\n",
        "#     for i in range(9):\n",
        "#         ax = plt.subplot(3, 3, i + 1)\n",
        "#         plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "#         plt.title(class_names[labels[i]])\n",
        "#         plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "Kr0BWyjBRaRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pretrained weights from ImageNet by specifying weights='imagenet'\n",
        "resent_model = ResNet50V2(include_top=False, weights='imagenet', input_shape=IMAGE_SIZE + [3])\n",
        "resent_model.summary()"
      ],
      "metadata": {
        "id": "hDxcehjsM647"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want, we can also only freeze first 3/5 portion of layers\n",
        "# for layer in resent_model.layers:\n",
        "#     layer.trainable = False\n",
        "# nb_layers = len(resent_model.layers)\n",
        "# print(resent_model.layers[nb_layers - 2].name)\n",
        "# print(resent_model.layers[nb_layers - 1].name)"
      ],
      "metadata": {
        "id": "2cHKMNx5N_WX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## another way##\n",
        "# # Freeze all the layers before the `fine_tune_at` layer\n",
        "# for layer in base_model.layers[:fine_tune_at]:\n",
        "#     #print('Layer ' + layer.name + ' frozen.')\n",
        "#     layer.trainable = None\n",
        "    \n",
        "resent_model.trainable = True\n",
        "for layer in resent_model.layers:\n",
        "    if layer.name == 'conv3_block4_out':\n",
        "        break\n",
        "    layer.trainable = False\n",
        "    #print('Layer ' + layer.name + ' frozen.')"
      ],
      "metadata": {
        "id": "tz4_XazFXoHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the new Binary classification layers\n",
        "# use global avg pooling to summarize the info in each channel\n",
        "x = GlobalAveragePooling2D()(resent_model.output)\n",
        "#include dropout with probability of 0.2 to avoid overfitting\n",
        "x = Dropout(0.5)(x)\n",
        "# x = Flatten()(x)\n",
        "# x = Dense(512,activation='relu')(x)\n",
        "x = Dense(512,activation='relu', kernel_regularizer='l2')(x)\n",
        "x = Dense(2,activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=resent_model.input, outputs=x)"
      ],
      "metadata": {
        "id": "6aAEW1UHPnxA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "YYG9uT_vPveL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a BinaryCrossentropy loss function. Use from_logits=True\n",
        "loss_function= tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "# Define an Adam optimizer with a learning rate of 0.1 * base_learning_rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate*0.1)# 0.001\n",
        "# Use accuracy as evaluation metric\n",
        "metrics=['accuracy']"
      ],
      "metadata": {
        "id": "CJ2zg84pYZFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss= loss_function, #CHANGE\n",
        "    metrics = metrics\n",
        ")"
      ],
      "metadata": {
        "id": "R7KzR7HtODd8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    shear_range = 0.2,\n",
        "    horizontal_flip = True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255\n",
        ")"
      ],
      "metadata": {
        "id": "1dqo9BX7P8h2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = train_datagen.flow_from_directory(train_path,\n",
        "                                                 target_size = (150,150),\n",
        "                                                 batch_size = 128,\n",
        "                                                 class_mode = 'categorical')"
      ],
      "metadata": {
        "id": "gQ7wZ3K-P_sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_set = test_datagen.flow_from_directory(valid_path,\n",
        "                                               target_size = (150,150),\n",
        "                                               batch_size = 128,\n",
        "                                               class_mode = 'categorical')"
      ],
      "metadata": {
        "id": "nu4vCMWwQAon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine_tune_epochs = 5\n",
        "# total_epochs =  initial_epochs + fine_tune_epochs\n",
        "\n",
        "# history_fine = model.fit(train_dataset,\n",
        "#                          epochs=total_epochs,\n",
        "#                          initial_epoch=history.epoch[-1],\n",
        "#                          validation_data=validation_dataset)"
      ],
      "metadata": {
        "id": "RKKXcg9xYsoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(training_set,\n",
        "                validation_data = testing_set,\n",
        "                epochs = 20,\n",
        "                steps_per_epoch=len(training_set),\n",
        "                validation_steps=len(testing_set))"
      ],
      "metadata": {
        "id": "kKQlpr4AQC-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loss\n",
        "plt.plot(hist.history['loss'], label='train loss')\n",
        "plt.plot(hist.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')\n",
        "\n",
        "# plot the accuracy\n",
        "plt.plot(hist.history['accuracy'], label='train acc')\n",
        "plt.plot(hist.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')"
      ],
      "metadata": {
        "id": "NU-KPaJmQIoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model.save('ResNet_model.h5')"
      ],
      "metadata": {
        "id": "A0JInWj2QKgn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}